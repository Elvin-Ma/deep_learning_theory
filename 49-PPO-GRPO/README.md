# 1 强化学习概念介绍
强化学习是机器学习的重要分支，其核心思想是智能体（Agent）通过**与环境的交互**学习最优策略，通过**试错机制**最大化`累积`奖励。以下是详细解析：<br>

# 1.1 强化学习核心概念
1. 交互式学习机制

强化学习强调智能体与环境的动态交互，通过奖励信号（正/负反馈）调整策略。例如自动驾驶汽车通过环境反馈（碰撞惩罚、安全行驶奖励）优化决策45。

2. 关键组成要素

- 策略（Policy）：从状态到动作的映射，可分为确定性策略（如直接选择最优动作）或随机策略（如概率选择）。

- 奖励（Reward）：环境反馈的标量值，用于评价动作效果（如游戏得分、任务完成度）。

- 价值函数（Value Function）：`评估长期累积奖励的预期`，如状态价值函数（某状态的未来收益）和动作价值函数（某状态下执行某动作的未来收益）。

- 环境模型（Model）：可选组件，用于预测状态转移和奖励（Model-based方法），若无模型则直接依赖经验（Model-free方法）。

- 与监督学习的区别: 监督学习依赖静态数据集和标签，而强化学习通过动态试错获取数据，更适用于序列决策问题（如机器人控制、游戏策略）14。


# 1.2 强化学习常用方法
强化学习的常用学习方法可分为三大类：<br>

>基于价值的方法;
>基于策略的方法;
>两者的结合混合方法（Actor-Critic）.

同时根据是否依赖环境模型可分为Model-Based与Model-Free方法。以下分类详解及Actor-Critic的归属说明：

### 1.2.1 基于价值的方法
核心思想：通过估计状态或状态-动作对的长期价值（如Q值）选择最优动作。
特点：

适合离散动作空间（如游戏中的按键选择）。
需维护价值表或价值网络。

### 1.2.2 基于策略的方法
核心思想：直接优化策略函数（如神经网络），输出动作概率分布。
特点：

适合连续动作空间（如机器人运动控制）。
通过策略梯度更新参数。

### 1.2.3 混合方法（Actor-Critic）
核心思想：结合策略网络（Actor）和价值网络（Critic），Actor生成动作，Critic评估动作价值，实现单步更新和稳定学习。
特点：

平衡策略梯度与价值估计，提升效率和稳定性。
适用于复杂任务（如机器人控制、游戏策略）。

### 1.2.4 Model-Based与Model-Free方法
Model-Based：建立环境模型预测状态转移（如AlphaGo的蒙特卡洛树搜索）。
Model-Free：直接通过交互经验学习（如Q-Learning、Actor-Critic）。
